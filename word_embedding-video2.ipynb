{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickle object from `subtitle_processing.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = load_obj(\"processed_video_subtitle2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subtitles = list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "videonames = list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = subtitles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_word_list(list_of_words):\n",
    "    \"\"\"\n",
    "    Given a list of words, remove stop words, punctuation, and empty strings\n",
    "    \"\"\"\n",
    "    STOPWORDS = stopwords.words(\"english\")\n",
    "    word_list = [w.lower().strip(string.punctuation) for w in list_of_words]\n",
    "    word_list = [w for w in word_list if w not in STOPWORDS and w!='']\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'everyone', 'phil', 'wong', 'fu', 'want', 'say', 'long', 'time', 'see']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_word_list(test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(filename):\n",
    "    \"\"\"\n",
    "    Given filename, load glove vector into a dictionary where the key is the word\n",
    "    and the value is a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    f = open(filename)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split(' ')\n",
    "        word = words[0]\n",
    "        vector = np.array(words[1:], dtype = 'float')\n",
    "        d[word] = vector\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use https://nlp.stanford.edu/projects/glove/ 300-dimensional vectors trained on Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = load_glove(\"glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04656  ,  0.21318  , -0.0074364, -0.45854  , -0.035639 ,\n",
       "        0.23643  , -0.28836  ,  0.21521  , -0.13486  , -1.6413   ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['the'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def subtitle2vec(subtitletext, gloves):\n",
    "    \"\"\"\n",
    "    Compute the average word embedding of the subtitles. Note that if the word\n",
    "    is not in the glove dictionary, it will be ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    subtitlevec = [gloves[word] for word in subtitletext if word in gloves]\n",
    "\n",
    "    lengthvec = len(subtitlevec)\n",
    "    centroid = [sum(x)/lengthvec for x in zip(*subtitlevec)]\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, subtitle in enumerate(subtitles):\n",
    "    embedding_dict[videonames[i]] = np.array(subtitle2vec(process_word_list(subtitle), glove)).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9244825430387438"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embedding_dict[videonames[0]], embedding_dict[videonames[1]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cosine_sim(v1,v2):\n",
    "    \"\"\"\n",
    "    Given two numpy arrays, compute cosine similarity and output an integer\n",
    "    \"\"\"\n",
    "    return cosine_similarity(v1, v2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest_video(video, n):\n",
    "    \"\"\"\n",
    "    Given a video subtitle file, recommend the closest n video based on the subtitle content\n",
    "    \"\"\"\n",
    "    cosine_sim_list = []\n",
    "    for i, subtitle in enumerate(subtitles):\n",
    "        cosine_sim_list.append(((videonames[i], i), compute_cosine_sim(embedding_dict[video], embedding_dict[videonames[i]])))\n",
    "    \n",
    "    cosine_sim_list.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return video, cosine_sim_list[1:n+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def recommend(select_video, n):\n",
    "    input_video, recommended = closest_video(select_video, n)\n",
    "    print(\"For the video:\" + input_video + \",\")\n",
    "    print(\"https://www.youtube.com/watch?v=\" + re.findall(r'\\-([\\d\\w]+)\\.en.vtt', input_video)[0])\n",
    "    print(\"\\nYour recommended videos are:\\n\")\n",
    "    \n",
    "    for video, score in recommended:\n",
    "        print(\"Video Number %s\" % video[1], ':', video[0], '|', str(np.round(score, 3)))\n",
    "        print(\"https://www.youtube.com/watch?v=\" + re.findall(r'\\-([\\d\\w]+)\\.en.vtt', video[0])[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the video:How Many People Know Wong Fu-g7nod5mS46Y.en.vtt,\n",
      "https://www.youtube.com/watch?v=g7nod5mS46Y\n",
      "\n",
      "Your recommended videos are:\n",
      "\n",
      "Video Number 74 : COMMENTS IN CARS - 'Untouchable'-VVTiiMEq4pE.en.vtt | 0.971\n",
      "https://www.youtube.com/watch?v=VVTiiMEq4pE \n",
      "\n",
      "Video Number 86 : The Long Lost Member-f9WVtmQvM0I.en.vtt | 0.969\n",
      "https://www.youtube.com/watch?v=f9WVtmQvM0I \n",
      "\n",
      "Video Number 76 : Asian Bachelorette-ag1IisyP1ak.en.vtt | 0.965\n",
      "https://www.youtube.com/watch?v=ag1IisyP1ak \n",
      "\n",
      "Video Number 15 : Goodbye 3 Million Subscribers-rjlxc-vxNBA.en.vtt | 0.964\n",
      "https://www.youtube.com/watch?v=vxNBA \n",
      "\n",
      "Video Number 14 : Girls Can't Take a Hint!-3DSYNfFu_NM.en.vtt | 0.963\n",
      "https://www.youtube.com/watch?v=3DSYNfFu_NM \n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend(videonames[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the video:Away We Happened - Ep 2-9Ka0aGyFGOk.en.vtt,\n",
      "https://www.youtube.com/watch?v=9Ka0aGyFGOk\n",
      "\n",
      "Your recommended videos are:\n",
      "\n",
      "Video Number 92 : Away We Happened - Ep 5-uNkx6OKoUME.en.vtt | 0.984\n",
      "https://www.youtube.com/watch?v=uNkx6OKoUME \n",
      "\n",
      "Video Number 96 : Away We Happened - Ep 4-tkPd-alWwmA.en.vtt | 0.982\n",
      "https://www.youtube.com/watch?v=alWwmA \n",
      "\n",
      "Video Number 97 : Away We Happened - Ep 3-jh4hjR-s7hY.en.vtt | 0.981\n",
      "https://www.youtube.com/watch?v=s7hY \n",
      "\n",
      "Video Number 48 : Just Another Nice Guy - Part 1-yU58jrx4pXs.en.vtt | 0.981\n",
      "https://www.youtube.com/watch?v=yU58jrx4pXs \n",
      "\n",
      "Video Number 2 : From Here On Out-RboSq7vxKqs.en.vtt | 0.981\n",
      "https://www.youtube.com/watch?v=RboSq7vxKqs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend(videonames[99], 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
